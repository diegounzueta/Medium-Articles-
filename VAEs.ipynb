{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed5ddea-340b-432e-8235-0e82323cb0fd",
   "metadata": {},
   "source": [
    "# Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5231c2-fb6a-4fdd-9335-388f5bbd2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.autonotebook import tqdm\n",
    "from itertools import chain\n",
    "\n",
    "import matplotlib.gridspec as gridspec    \n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33a9a9-55e5-428a-a0c7-ab8b924e4373",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 256\n",
    "image_dim = 784 \n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # convert to tensor\n",
    "    transforms.Lambda(lambda x: x.view(image_dim)) # flatten into vector\n",
    "    ])\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9617b3b-b904-442c-8755-20f47032956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    simple encoder with a single hidden dense layer (ReLU activation)\n",
    "    and linear projections to the diag-Gauss parameters\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.fc2_sigma = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        mu = self.fc2(x)\n",
    "        logsigma = self.fc2_sigma(x)\n",
    "        return(mu, logsigma)\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    simple decoder: single dense hidden layer (ReLU activation) followed by \n",
    "    output layer with a sigmoid to squish values\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ece1f2-f936-401a-ae4c-01e75fa73a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6ac577-c36f-4c7f-bcd0-744a623b5bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(mu, log_sigma2):\n",
    "\n",
    "    eps = torch.randn(mu.shape[0], mu.shape[1])\n",
    "\n",
    "    return mu + torch.exp(log_sigma2 / 2) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a114b4a-93d1-4792-b396-a7eb3ec115e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 2\n",
    "enc_hidden_units = 512\n",
    "dec_hidden_units = 512\n",
    "nEpoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213cf380-d952-4c53-9ce1-0db1b8c96916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# construct the encoder, decoder and optimiser\n",
    "enc = Encoder(image_dim, enc_hidden_units, embedding_dim)\n",
    "dec = Decoder(embedding_dim, dec_hidden_units, image_dim)\n",
    "optimizer = optim.Adam(chain(enc.parameters(), dec.parameters()), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde2312a-abec-451a-ad90-82a76e96e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_plot = []\n",
    "# training loop\n",
    "for epoch in range(nEpoch):\n",
    "    losses = []\n",
    "    trainloader = tqdm(train_loader)\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, _ = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mu, log_sigma2 = enc(inputs)\n",
    "        #print(inputs.shape, mu.shape)\n",
    "        z = sample(mu, log_sigma2)\n",
    "        #print(\"sample shape: \", z.shape)\n",
    "        outputs = dec(z)\n",
    "\n",
    "        # E[log P(X|z)] - as images are binary it makes most sense to use binary cross entropy\n",
    "        # we need to be a little careful - by default torch averages over every observation \n",
    "        # (e.g. each  pixel in each image of each batch), whereas we want the average over entire\n",
    "        # images instead\n",
    "        recon = F.binary_cross_entropy(outputs, inputs, reduction='sum') / inputs.shape[0]\n",
    "        \n",
    "\n",
    "        # kl = D_KL(Q(z|X) || P(z|X)) - calculate in closed form\n",
    "        kl = torch.mean(-0.5 * torch.sum(1 + log_sigma2 - mu ** 2 - log_sigma2.exp(), dim = 1), dim = 0)\n",
    "        loss = recon + kl\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of the loss and update the stats\n",
    "        losses.append(loss.item())\n",
    "        trainloader.set_postfix(loss=np.mean(losses), epoch=epoch)\n",
    "    \n",
    "    loss_plot.append(np.average(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90788f6-cd6d-4ae5-b7c4-0a0e9afdc621",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_d = outputs.detach().numpy()\n",
    "outputs_d.shape = (96, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85761bf0-52f1-4e4e-87cb-f04c070e1a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "  plt.subplot(int(str(24)+str(i+1)))\n",
    "  plt.imshow(train_set.train_data[i], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "\n",
    "  plt.subplot(int(str(24)+str(i + 5)))\n",
    "  plt.imshow(outputs_d[i], cmap=plt.get_cmap('gray'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb31aa0-146c-4e93-a6b5-4fb8a7c3e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.shape, log_sigma2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a2f80-65ca-457a-b0ef-b87e6749ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = mu[0].detach().numpy()\n",
    "sigma = log_sigma2[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d6e2b-b9e5-4593-be22-66bd8b3ec9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3b0e59-0fd4-4f44-829d-158eea9343aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.linspace(mean[0] + sigma[0] *4, mean[0] - sigma[0]*4, 21)\n",
    "y = np.linspace(mean[1] - sigma[1] *4, mean[1] + sigma[1]*4, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca069d0-e69a-41f8-869d-84d792b76bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323e60b-0401-4b98-bbd0-5813e04ebd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_img = np.zeros((28*21, 28*21))\n",
    "\n",
    "size = 21\n",
    "\n",
    "for row in range(size):\n",
    "  for col in range(size):\n",
    "    out = dec(torch.Tensor([x[row],  y[col]])).detach().numpy().reshape(28, 28)\n",
    "    \n",
    "    sample_img[col*28:col*28+28, row*28: row*28+28] = out\n",
    "    # axs[row, col].axis('off')\n",
    "    # axs[row, col].imshow(out, cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee780d98-f339-45e8-a5d7-8071c937c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.figure(figsize = (10, 10))\n",
    " plt.axis(\"off\")\n",
    " plt.imshow(sample_img, cmap=plt.get_cmap('gray'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
